source:
  url: "https://arxiv.org/abs/2407.13578"
  title: "How Reliable are LLMs as Knowledge Bases? Re-thinking Factuality and Consistency"
  type: "paper"
  accessed: "2025-12-04"

summary:
  one_liner: "Proposes a framework for evaluating LLMs as Knowledge Bases based on Factuality (accuracy on seen/unseen data) and Consistency (stability of answers)."
  key_concepts:
    - "Factuality"
    - "Consistency"
    - "Reliability Score"
    - "Unseen Knowledge"
    - "Net Consistently Correct Rate (NCCR)"
    - "Inconsistent/Uninformative Rate (IUR)"
  architecture: "Evaluation framework using 'UnseenQA' dataset and consistency checks via multiple-choice questions with distractors."

relevance_to_haios:
  output_pipeline: "Provides metrics to validate the quality of the output. Before 'publishing' an epoch, we can run consistency checks to ensure reliability."
  feedback_capture: "The 'Consistency' check (asking variations of the same question) is a self-feedback mechanism to detect hallucinations or unstable knowledge."
  epoch_management: "Defines 'Reliability' for a KB. We can use NCCR/IUR as acceptance criteria for promoting a new Epoch."
  utility_increase: "Higher reliability = higher utility. This paper gives us the math to measure it."
  relevance_score: 5

technical_patterns:
  - name: "Consistency Check via Distractors"
    description: "Generate multiple-choice questions with distractors similar to the model's response to measure stability."
    code_example: "Cons(q,r) = 1/N * Sum(Match(Ri, r))"
    applicable_to: "Validation Agent / Output Pipeline"
  - name: "Unseen Knowledge Handling"
    description: "Explicitly testing for 'uninformative' responses on unseen data to prevent hallucination."
    code_example: "IUR metric calculation"
    applicable_to: "Refinement Layer (handling new logs)"

gaps:
  - "Focuses on static evaluation, not dynamic evolution of the KB."
  - "Does not address how to *fix* inconsistencies, only how to measure them."

quotes:
  - text: "For unseen knowledge, a factual LLM should demonstrate a high uninformative rate."
    context: "Crucial for HAIOS: If we don't know, we should say 'I don't know' (or 'doxa') rather than hallucinating 'episteme'."
  - text: "We argue that it may be too strict to expect LLMs to be always consistent... we propose to monitor model behavior through post-processing."
    context: "Validates our approach of using a 'Refinement Layer' (post-processing) rather than expecting the raw LLM to be perfect."
